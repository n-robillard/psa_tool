{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# General description of GSA2PB\n",
    "\n",
    "## Introduction\n",
    "\n",
    "GSA2PB have the main objetive to implement a analyse the allosteric communication, based on a structural alphabet, the Protein Blocks (PBs). The model of this program is GSA-tools, developped by Fraternalli *et al.* in 2013 (article), which analyze the trajectories of molecular dynamique by the use of a strucural alphabet. This approach was based on a succession of statistic analyze for predict the allosteric mouvment in a protein. However, GSA-tools use a old version of Gromacs, and was not totally useable.\n",
    "\n",
    "Here we implement the same statistic analyse but based on the PBs alphabet, which are provide by a free use software : PBxplore."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I) Select the specific pdb file and compute PBxplore assignement\n",
    "\n",
    "In the first part of the program, we need to assign the Protein Block alphabet to each pdb file. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II) Parsing the fasta file and make it in a right form\n",
    "\n",
    "After compute the PBassign of PBxplore, we need to import the sequences in the fasta file into Python. To do that, we use the `SeqIO` fonction of `biopython`. We import all the sequences into a `numpy` array, named `fasta_parsed`, with one sequnces in :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "\n",
    "def parse_fasta(pathToFile):\n",
    "    allSeqs = []\n",
    "    for seq_record in SeqIO.parse(pathToFile, \"\"\"fasta\"\"\"):\n",
    "            allSeqs.append(seq_record.seq)\n",
    "    pb_seq = np.array(allSeqs)\n",
    "    return pb_seq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "pathToFile = open(\"../data/sequences.PB.fasta\")\n",
    "\n",
    "fasta_parsed = parse_fasta(pathToFile)\n",
    "print(fasta_parsed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Z' 'Z' 'o' ... 'd' 'Z' 'Z']\n",
      " ['Z' 'Z' 'o' ... 'd' 'Z' 'Z']\n",
      " ['Z' 'Z' 'l' ... 'd' 'Z' 'Z']\n",
      " ...\n",
      " ['Z' 'Z' 'a' ... 'd' 'Z' 'Z']\n",
      " ['Z' 'Z' 'd' ... 'd' 'Z' 'Z']\n",
      " ['Z' 'Z' 'b' ... 'd' 'Z' 'Z']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The fasta file is imported in python but have some defaults: first, we have one frame by rows and columns represent each fragments. For make the statistic analyze more easier, we transpose the matrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def transpose_fasta(sequences):\n",
    "    sequences = sequences.transpose()\n",
    "    return sequences\n",
    "\n",
    "fasta_transposed = transpose_fasta(fasta_parsed)\n",
    "print(fasta_transposed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Z' 'Z' 'Z' ... 'Z' 'Z' 'Z']\n",
      " ['Z' 'Z' 'Z' ... 'Z' 'Z' 'Z']\n",
      " ['o' 'o' 'l' ... 'a' 'd' 'b']\n",
      " ...\n",
      " ['d' 'd' 'd' ... 'd' 'd' 'd']\n",
      " ['Z' 'Z' 'Z' ... 'Z' 'Z' 'Z']\n",
      " ['Z' 'Z' 'Z' ... 'Z' 'Z' 'Z']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One more default of this array is inherent to the PBxplore : the 'Z' \"trajectory\". PBxplore analyze the trajectories of each amino acide *n* by the relation with amino acids *n-2*, *n-1*, *n+1*, *n+2*, which are impossible for the the two first and twor last amino acids. Even the calculation statistics parameters aren't possible (cause of the ordinal placment, see more in details in the statistic part), it's preferable to eliminated the two first and two last fragments, wich reduce a little the time of calculation. More of that, the 'Z' not represent a trajectory, but the impossibility of calculate it (as NA or NaN represent the absence of value), so it's not macking sense to calculated statistics on thats fragments."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "fasta_formed = fasta_transposed[2:-2]\n",
    "print(fasta_formed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['o' 'o' 'l' ... 'a' 'd' 'b']\n",
      " ['p' 'p' 'c' ... 'f' 'j' 'f']\n",
      " ['a' 'a' 'd' ... 'k' 'k' 'k']\n",
      " ...\n",
      " ['d' 'd' 'd' ... 'd' 'd' 'd']\n",
      " ['d' 'd' 'd' ... 'd' 'd' 'd']\n",
      " ['d' 'd' 'd' ... 'd' 'd' 'd']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point, we have a now a formated numpy array, wich contained a every trajectory for each fragments in rows. At this point, we can perform statistical analysis to determine the correlation between trajectories.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III) Statistical analysis on trajectories\n",
    "\n",
    "In the article of Pandini *et al.*, they use a statistical model to determined the correlation between local motions. In this aspect they determined a normalized mutual information, based on three different statistics information : the mutual information, the joint entropy and the expected error of the mutual information. Here, we implement the same approach to our strucural alphabet."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) The mutual information\n",
    "\n",
    "The first statistical parameter is the mutual information. In article of Pandini *et al* and in the GSA tools software, they calculated the mutual information *I(X;Y)* as the following equation:\n",
    "\n",
    "$\n",
    "I(X;Y)=\\sum_{x\\in X}\\sum_{y\\in Y} p(x,y)log_2(\\frac{p(x,y)}{p_1(x) p_2(y)})\n",
    "$\n",
    "\n",
    "To implement that, we defined a fonction `calculate_MI` as:\n",
    "\n",
    "```python\n",
    "def calulate_MI(px, py, pxy):\n",
    "    size = len(px)\n",
    "        MI = 0\n",
    "        for i in range(0,size):\n",
    "            for j in range(0,size):\n",
    "                if pxy[i,j] != 0:\n",
    "                    MI += pxy[i,j] * math.log2(pxy[i,j]/(px[i]*py[j]))\n",
    "        if MI > 0:\n",
    "            return MI\n",
    "        else:\n",
    "            return 0 \n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) The joint entropy\n",
    "\n",
    "On the same approach, the joint entropy is defined as *H(X;Y)*:\n",
    "\n",
    "$\n",
    "H(X;Y)=\\sum_{x\\in X}\\sum_{y\\in Y} p(x,y)log_2 p(x,y)\n",
    "$\n",
    "\n",
    "```python\n",
    "def calculate_joint_entropy(pxy):\n",
    "    size = len(pxy)\n",
    "    joint_entropy = 0\n",
    "    for i in range(0,size):\n",
    "        for j in range(0,size):\n",
    "            if pxy [i,j] !=0:\n",
    "                joint_entropy += pxy[i,j] * math.log2(pxy[i,j])\n",
    "    if joint_entropy > 0:\n",
    "        return joint_entropy\n",
    "    else:\n",
    "        return 0\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3) The expected error of the mutual information\n",
    "\n",
    "The third parameter needed for the normalized mutual information is the expected error. In fact, the calculation of the information theoretical quantities on a finite size sample is affected by random errors, inherent to the sample, which are negligeable in the case of Shanon's entropy, but affect the mutual information calculs. In this case, Roulston *et al.* () present a method to improve this calcul, which used the expect error on the MI. This error is calculated as:\n",
    "\n",
    "$\n",
    "\\varepsilon (X;Y) = \\frac{B^*_{XY} - B^*_X - B^*_Y +1}{2N}\n",
    "$\n",
    "\n",
    "Where $B^*_{XY}$ , $B^*_X$ , $B^*_Y$ are the count of probability different of zero in the stat *(X;Y)*, *X* and *Y*, respectively.\n",
    "\n",
    "On this purpose, we defiened a fonction which calculate this expected error as following:\n",
    "\n",
    "```python\n",
    "def calculate_eeMI(bxy, bx, by, n):\n",
    "    return (bxy - bx - by + 1)/(2 * n)\n",
    "```\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4) The normalized mutual information\n",
    "\n",
    "After calculate all of this statistical data,we obtained three differents matrixes, which have the same lenght than the sequences of the protein in row and columns. Each matrixes indicate the mutual information, the joint entropy and the expected error for each fragment against each other fragment. Then, we can calculated the normalized mutual information, defined as following:\n",
    "\n",
    "$ \n",
    "I^n_{LL}(C_i;C_j)=\\frac{I(C_i;C_j) - \\varepsilon(C_i;C_j)}{H(C_i;C_j)}\n",
    "$\n",
    "\n",
    "For calculate this, we do a one fonction which aim to do the mutual information normalized and every other part of the statistic analysis:\n",
    "\n",
    "``` python\n",
    "def calculate_MI_global(frames_all):\n",
    "    seq_size = len(frames_all[:,0])\n",
    "    weight = 1 / seq_size\n",
    "    MI = np.zeros((seq_size,seq_size))\n",
    "    joint_entropy = np.zeros((seq_size,seq_size))\n",
    "    eeMI = np.zeros((seq_size,seq_size))\n",
    "    normalized_MI = np.zeros((seq_size,seq_size))\n",
    "    for i in range(0,len(frames_all)):\n",
    "        for j in range(i,len(frames_all)):\n",
    "            px = calculate_p(frames_all[i],weight) \n",
    "            py = calculate_p(frames_all[j],weight)\n",
    "            pxy = calculate_pxy(frames_all[i],frames_all[j],weight)\n",
    "            MI[i, j] = calulate_MI(px, py, pxy)\n",
    "            joint_entropy[i, j] = calculate_joint_entropy(pxy)\n",
    "            bx = np.count_nonzero(px)\n",
    "            by = np.count_nonzero(py)\n",
    "            bxy = np.count_nonzero(pxy)\n",
    "            eeMI[i, j] = calculate_eeMI(bxy, bx, by, seq_size)\n",
    "            if joint_entropy[i, j] != 0:\n",
    "                normalized_MI[i, j] = (MI[i, j] - eeMI[i, j])/joint_entropy[i, j]\n",
    "            else:\n",
    "                normalized_MI[i, j] = 0\n",
    "            if i != j:\n",
    "                MI[j, i]=MI[i, j]\n",
    "                eeMI[j, i]=eeMI[i, j]\n",
    "                joint_entropy[j, i]=joint_entropy[i, j]\n",
    "                normalized_MI[j, i] = normalized_MI[i, j]\n",
    "    return MI, eeMI, joint_entropy, normalized_MI\n",
    "```\n",
    "\n",
    "In this fonction, only the numpy array of the fasta is needed to compute all of the model, and return the mutual information, the joint entropy, the mutual information and his expected error. every part of this analysis is wright in the `pb_analyze.py` file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from pb_analyze import pba\n",
    "\n",
    "MI, eeMI, joint_entropy, normalized_MI = pba.calculate_MI_global(fasta_formed)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pb_analyze'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52595/3822129155.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpb_analyze\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meeMI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_MI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_MI_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasta_formed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pb_analyze'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(MI)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(eeMI)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(joint_entropy)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(normalized_MI)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IV) Vizualisaton\n",
    "\n",
    "In this last part, we add a graphic representation for matrixes with the use of `matplotlib` and `seaborn`."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('GSA2PB_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "5c7ffd004007d3d55907443578e30509ba42393fb2624d1ca3d49895f63bea82"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}