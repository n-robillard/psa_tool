"""
Statisitic analysis for the determiation of allosteric communiation

===================================================================

Analysis tools for allosteric movements between protein blocks generated by
PBxplore, based on the same analyse do by GSA-tools with Gromacs related 
structural alphabet.

This part of the program target to calculate the joint entropy, the mutual information
and the expected error of this mutual information between two columns in the parse fasta
(refer to the script #FIXME). This three statistic parameters permit to calculate the 
normalized mutual information, and make an idea of the correlated local motion in a
protein.

The principe of the calcul are explained in the artical of Pandini et al., 2012 in the
FASEB J., and they are detailed in the notebook present in /doc
"""

import numpy as np
from scipy import stats
import math

def calculate_MI_global(frames_all): #FIXME: propably an other name for this calcul
    """calculate normalized mutual information and others needed parameters for it:

    - frames_all (numpy array): parsed fasta into Python (preferably transposed)

    return: normalized mutual information, mutual information, joint entropy, mutual information expected error
    """
    seq_size = len(frames_all[:,0])
    weight = 1 / seq_size #
    MI = np.zeros((seq_size,seq_size))
    joint_entropy = np.zeros((seq_size,seq_size))
    eeMI = np.zeros((seq_size,seq_size))
    normalized_MI = np.zeros((seq_size,seq_size))
    for i in range(0,len(frames_all)):
        for j in range(i,len(frames_all)):
            px = calculate_p(frames_all[i],weight) 
            py = calculate_p(frames_all[j],weight)
            pxy = calculate_pxy(frames_all[i],frames_all[j],weight)
            MI[i, j] = calulate_MI(px, py, pxy)
            joint_entropy[i, j] = calculate_joint_entropy(pxy)
            bx = np.count_nonzero(px)
            by = np.count_nonzero(py)
            bxy = np.count_nonzero(pxy)
            eeMI[i, j] = calculate_eeMI(bxy, bx, by, seq_size)
            if joint_entropy[i, j] != 0:
                normalized_MI[i, j] = (MI[i, j] - eeMI[i, j])/joint_entropy[i, j]
            else:
                normalized_MI[i, j] = 0
            if i != j:
                MI[j, i]=MI[i, j]
                eeMI[j, i]=eeMI[i, j]
                joint_entropy[j, i]=joint_entropy[i, j]
                normalized_MI[j, i] = normalized_MI[i, j]
    return MI, eeMI, joint_entropy, normalized_MI

def calculate_p(frame, weight): 
    """calculate the probability of each letter in the PBs and integre it in list

    - frame (matrix 2d): 
    - weight (float):

    return (matrix): 
    """
    matrix= np.zeros((16))
    for i in range(0,len(frame)):
        # discretisation of the alphabet in int number (based on ASCII coding) and
        index_row = ord(frame[i])-97
        matrix[index_row] += weight
    return matrix


def calculate_pxy(frame_src, frame_target, weight):
    """calculate the probability of 
    """
    matrix= np.zeros((16,16))
    for i in range(0,len(frame_src)):
        index_row = ord(frame_src[i])-97
        index_col = ord(frame_target[i])-97
        matrix[index_row,index_col] += weight
    return matrix



def calulate_MI(px, py, pxy):
    """calculate the mutual information between to matrix:

    - px (matrix) : probability matrix for the i index
    - py (matrix): probalbity matrix or i+n index
    - pxy (matrix): probability 

    return (matrix): mutual information of probability matrixes
    """
    size = len(px)
    MI = 0
    for i in range(0,size):
        for j in range(0,size):
            if pxy[i,j] != 0:
                MI += pxy[i,j] * math.log2(pxy[i,j]/(px[i]*py[j]))
    if MI > 0:
        return MI
    else:
        return 0            

def calculate_joint_entropy(pxy):
    """Calculate the joint entropy with the probability matrix:

    - pxy (matrix): probability

    return (matrix): joint entropy of probability 
    """
    size = len(pxy)
    joint_entropy = 0
    for i in range(0,size):
        for j in range(0,size):
            if pxy [i,j] !=0:
                joint_entropy += pxy[i,j] * math.log2(pxy[i,j])
    if joint_entropy > 0:
        return joint_entropy
    else:
        return 0

def calculate_eeMI(bxy, bx, by, n):
    """Calculate the expected error of the mutual information
    - bxy (float): count of probability different to zero in matrix pxy
    - bx (float): count of probability different to zero in matrix pxy
    - bx (float): count of probability different to zero in matrix pxy
    - n (int): size of the matrix of probability, or number of fragments in the original sequence

    return (float): expected error of the mutual information
    """
    return (bxy - bx - by + 1)/(2 * n)